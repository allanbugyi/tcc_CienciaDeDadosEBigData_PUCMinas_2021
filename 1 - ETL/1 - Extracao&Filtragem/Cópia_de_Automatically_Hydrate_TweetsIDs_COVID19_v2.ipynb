{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xQiik2HFUtqL"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "<center><b>© 2020. Content is made available under the CC-BY-NC-ND 4.0 license. Christian Lopez, lopezbec@lafayette.edu/  Malolan Vasu, vasum@lafayette.edu <b><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TwMczOoPVH6e"
   },
   "source": [
    "**UPDATED ON 10-18-2020**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mmFBUngZaxt7"
   },
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/lopezbec/COVID19_Tweets_Dataset/blob/master/Automatically_Hydrate_TweetsIDs_COVID19_v2.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wrI81Xn6wh0d"
   },
   "source": [
    "# **Notebook to automatically \"hydrate\" tweets-ID**\n",
    "\n",
    "This notebook will allow you to automatically hydrate the tweets-ID from our [COVID19_Tweets_dataset GitHub repository](https://github.com/lopezbec/COVID19_Tweets_Dataset).\n",
    "\n",
    "\n",
    "You can run this notebook directly on the cloud using Google Colab [(see how to tutorials)]( https://colab.research.google.com/notebooks/welcome.ipynb#scrollTo=xitplqMNk_Hc) and Google Drive.\n",
    "\n",
    "In order to hydrate the tweet-IDs using [TWARC](https://github.com/DocNow/twarc) you need to create a [Twitter Developer Account]( https://developer.twitter.com/en/apply-for-access).\n",
    "\n",
    "First run the Initialization block to set up the required libraries. Then, run the Configuration block and choose your required configuration for tweet IDs to hydrate (dates needed, keywords needed). Finally, run the Hydrate block.  In the case that the code unexpectedly stops in the hydrate phase, you only need to run Initialization and Hydration. You aren't required to run configuration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DbVmAxxGHUjO"
   },
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G-Zr5kB6wknZ"
   },
   "source": [
    "### Mount Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dHzFzMoOV0tu"
   },
   "source": [
    "The code will clone part the repository and place it in your Google drive. Also, the notebook will place all the hydrated tweets in your Google Drive. Here you need to type where in your Google Drive you would like the information stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aeimRLrjwpRr",
    "outputId": "f4793559-99bc-4a9e-f175-4370bd0709e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current directory is : /media/allan/Seagate Expansion Drive/Allan/tcc/COVID19_Tweets/Summary_Details_files\n"
     ]
    }
   ],
   "source": [
    "#@title Set up Directory { run: \"auto\"}\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "#from google.colab import drive \n",
    "from IPython.display import clear_output\n",
    "#drive.mount('/content/gdrive')\n",
    "#working_directory = 'My Drive/COVID19_Tweets' #@param {type:\"string\"}\n",
    "working_directory = '/media/allan/Seagate Expansion Drive/Allan/tcc/COVID19_Tweets/Summary_Details_files'\n",
    "#wd=\"/content/gdrive/\"+working_directory\n",
    "#os.chdir(wd)\n",
    "os.chdir(working_directory)\n",
    "\n",
    "dirpath = os.getcwd()\n",
    "print(\"current directory is : \" + dirpath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RDzd7FUKFviv"
   },
   "source": [
    "### Install twarc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jGvHm-ggFubK"
   },
   "outputs": [],
   "source": [
    "%pip install twarc\n",
    "%pip install jsonlines\n",
    "%pip install wget\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1gBPP8oJGDqM",
    "outputId": "2c156acb-6450-4bf6-8c72-a78a8bfdd086"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: twarc\n",
      "Version: 2.3.1\n",
      "Summary: Archive tweets from the command line\n",
      "Home-page: https://github.com/docnow/twarc\n",
      "Author: Ed Summers\n",
      "Author-email: ehs@pobox.com\n",
      "License: UNKNOWN\n",
      "Location: /home/allan/.local/lib/python3.8/site-packages\n",
      "Requires: python-dateutil, click, requests-oauthlib, click-plugins, click-config-file\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Name: jsonlines\n",
      "Version: 2.0.0\n",
      "Summary: Library with helpers for the jsonlines file format\n",
      "Home-page: https://github.com/wbolster/jsonlines\n",
      "Author: Wouter Bolsterlee\n",
      "Author-email: wouter@bolsterl.ee\n",
      "License: BSD\n",
      "Location: /home/allan/.local/lib/python3.8/site-packages\n",
      "Requires: \n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#Check if TWARC was installed correctly on the Virtual Machine\n",
    "%pip show twarc\n",
    "%pip show jsonlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fEsz47hdFHtW"
   },
   "source": [
    "### Twitter API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "form",
    "id": "et0_5DEEFNpW"
   },
   "outputs": [],
   "source": [
    "#@title Insert API Keys here { run : \"auto\"}\n",
    "from twarc import Twarc\n",
    "\n",
    "# These keys are received after applying for a twitter developer account\n",
    "\n",
    "consumer_key = \"YUiL9Bv6SWdnLn9QdniCzvKwt\" #@param {type:\"string\"}\n",
    "consumer_secret = \"TIQrbc1yq0C9GJTOf87kxqBhbNUmwDRHjAYaKZo2KbdVf53WuP\" #@param {type:\"string\"}\n",
    "access_token = \"183222391-onqKn5n5iA0ZABGSn6MkANids3Z3faLS8z5HSK5t\" #@param {type:\"string\"}\n",
    "access_token_secret = \"kE93SxtEFs4ueM54h8rjKV6XTAa0XiQ5meSAKqehlOhjj\" #@param {type:\"string\"}\n",
    "\n",
    "t = Twarc(consumer_key, consumer_secret, access_token, access_token_secret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OyS66qo29K_g"
   },
   "source": [
    "### Download Github Files onto Drive for given dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YZiHM582ReTh"
   },
   "source": [
    "You need to frist indicate the date range of file you would like to hydrate. Rememeber that the Start_date cannot be before 2020-01-22. If a incorrect end date is selected it will be not considered.  At the end you will see how many files you are requesting to download \n",
    "\n",
    "(it is assumed you want all hours from a given day, if you dont want some hour, you will need to deleted the hours you wont want).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UeLOiUghTpKc",
    "outputId": "43f2e4d4-5868-47ff-ef8e-f15b0d779941"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have requested to download a total of 744 files \n",
      " The more files you download at once the longer it will take to upload to your Drive\n"
     ]
    }
   ],
   "source": [
    "#@title Date fields\n",
    "import pandas as pd\n",
    "Start_date = '2021-04-03' #@param {type:\"date\"}\n",
    "#@title Date fields\n",
    "End_date = '2021-05-03' #@param {type:\"date\"}\n",
    "\n",
    "\n",
    "\n",
    "datelist = pd.date_range(Start_date, End_date).tolist()\n",
    "number_files=0\n",
    "for date in datelist:\n",
    "  for h in ['00','01', '02','03','04','05','06','07','08','09','10','11','12','13','14','15','16','17','18','19','20','21','22','23']:\n",
    "    number_files=number_files+1\n",
    "\n",
    "print(\"You have requested to download a total of \"+ str(number_files) + ' files \\n The more files you download at once the longer it will take to upload to your Drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5VtnuqvTgIJO"
   },
   "source": [
    "The code below will create a new directory `Summary_Details_files` and save all the `.csv. files from the reposiory that were requested on that directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "PPbChi1mbVHv"
   },
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "import wget\n",
    "\n",
    "path = os.path.join(wd, \"Summary_Details_files\") \n",
    "if not os.path.exists('./Summary_Details_files'):\n",
    "  try:\n",
    "    os.makedirs(path)\n",
    "    os.chdir('Summary_Details_files')\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "datelist = pd.date_range(Start_date, End_date).tolist()\n",
    "files_list=[]\n",
    "for date in datelist:\n",
    "  day=date.strftime(\"%Y_%m_%d\")\n",
    "  p1=\"https://raw.githubusercontent.com/lopezbec/COVID19_Tweets_Dataset/master/Summary_Details/\"\n",
    "  m=day[0:7]\n",
    "  for h in ['00','01', '02','03','04','05','06','07','08','09','10','11','12','13','14','15','16','17','18','19','20','21','22','23']:\n",
    "    file=p1+m+\"/\"+day+\"_\"+str(h)+\"_Summary_Details.csv\"\n",
    "    wget.download(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ASdYyTqI9RUG"
   },
   "source": [
    "# Choose Settings Tweets_IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jo52kGhpk69s"
   },
   "source": [
    "This are all the files you requested:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "uLREJkIRjvDh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/allan/Seagate Expansion Drive/Allan/tcc/COVID19_Tweets/Summary_Details_files\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3165: DtypeWarning: Columns (6) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-49a3464f159a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m       \u001b[0;31m#frames = [Summary_Details, data]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m       \u001b[0;31m#Summary_Details = pd.concat(frames)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mSummary_Details\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSummary_Details\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mSummary_Details\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mappend\u001b[0;34m(self, other, ignore_index, verify_integrity, sort)\u001b[0m\n\u001b[1;32m   7980\u001b[0m             \u001b[0mto_concat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7981\u001b[0m         return (\n\u001b[0;32m-> 7982\u001b[0;31m             concat(\n\u001b[0m\u001b[1;32m   7983\u001b[0m                 \u001b[0mto_concat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7984\u001b[0m                 \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIndexes\u001b[0m \u001b[0mhave\u001b[0m \u001b[0moverlapping\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \"\"\"\n\u001b[0;32m--> 285\u001b[0;31m     op = _Concatenator(\n\u001b[0m\u001b[1;32m    286\u001b[0m         \u001b[0mobjs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m             \u001b[0;31m# consolidate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m             \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m             \u001b[0mndims\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_consolidate_inplace\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   5539\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconsolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5541\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_protect_consolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5543\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_protect_consolidate\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m   5527\u001b[0m         \"\"\"\n\u001b[1;32m   5528\u001b[0m         \u001b[0mblocks_before\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5529\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5530\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mblocks_before\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5531\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mf\u001b[0;34m()\u001b[0m\n\u001b[1;32m   5537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5538\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5539\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconsolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5541\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_protect_consolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mconsolidate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    986\u001b[0m         \u001b[0mbm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0mbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_consolidated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 988\u001b[0;31m         \u001b[0mbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    989\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m_consolidate_inplace\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    991\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_consolidated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 993\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_consolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    994\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_consolidated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_known_consolidated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m_consolidate\u001b[0;34m(blocks)\u001b[0m\n\u001b[1;32m   1914\u001b[0m     \u001b[0mnew_blocks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBlock\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1915\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_can_consolidate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_blocks\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrouper\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1916\u001b[0;31m         merged_blocks = _merge_blocks(\n\u001b[0m\u001b[1;32m   1917\u001b[0m             \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcan_consolidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_can_consolidate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m_merge_blocks\u001b[0;34m(blocks, dtype, can_consolidate)\u001b[0m\n\u001b[1;32m   1940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1941\u001b[0m         \u001b[0margsort\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_mgr_locs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1942\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1943\u001b[0m         \u001b[0mnew_mgr_locs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_mgr_locs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "\n",
    "Summary_Details=pd.DataFrame() \n",
    "directory =  os.getcwd()\n",
    "print(directory)\n",
    "counter =0\n",
    "for root,dirs,files in os.walk(directory):\n",
    "  for file in files:\n",
    "    print(counter)\n",
    "    counter+=1\n",
    "    if file.endswith(\".csv\"):  \n",
    "      data = pd.read_csv(directory+\"/\"+file) \n",
    "      #frames = [Summary_Details, data]\n",
    "      #Summary_Details = pd.concat(frames)\n",
    "    Summary_Details = Summary_Details.append(data, ignore_index=True)\n",
    "Summary_Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w0CfU-SUlUIp"
   },
   "source": [
    "### Filter Tweets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Fj9ydtn9aIL"
   },
   "source": [
    "You can filter the tweets by:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "qGdsBbsHxrcG",
    "outputId": "c3404c51-4736-4422-9f24-7674b00ee3de"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Summary_Details' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-9ae23c8c0ad3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mCountry_of_Origin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'BR'\u001b[0m \u001b[0;31m#@param {type:\"string\"}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mSummary_Details2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSummary_Details\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJust_English_Tweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mSummary_Details2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSummary_Details2\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mSummary_Details2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Language'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Summary_Details' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "Just_English_Tweets= False #@param {type:\"boolean\"}\n",
    "Just_Portuguese_Tweets = True #@param {type: \"boolean\"}\n",
    "Just_with_Geolocation_information = False #@param {type:\"boolean\"}\n",
    "Just_Original_tweets = True #@param {type:\"boolean\"}\n",
    "Minimun_No_Likes = 0 #@param {type:\"number\"}\n",
    "Minimun_No_Retweets =  0 #@param {type:\"number\"}\n",
    "Just_Country_Specific = True #@param {type:\"boolean\"}\n",
    "Country_of_Origin = 'BR' #@param {type:\"string\"}\n",
    "\n",
    "Summary_Details2=Summary_Details\n",
    "if(Just_English_Tweets):\n",
    "  Summary_Details2=Summary_Details2[ Summary_Details2['Language']=='en']\n",
    "if(Just_Portuguese_Tweets):\n",
    "    Summary_Details2=Summary_Details2[ Summary_Details2['Language']=='pt'] \n",
    "if(Just_with_Geolocation_information):\n",
    "  Summary_Details2=Summary_Details2[ Summary_Details2['Geolocation_coordinate']=='YES']\n",
    "if(Just_Original_tweets):\n",
    "  Summary_Details2=Summary_Details2[ Summary_Details2['RT']=='NO']\n",
    "if(Just_Country_Specific):\n",
    "    Summary_Details2=Summary_Details2[ Summary_Details2['Country']==Country_of_Origin]\n",
    "\n",
    "\n",
    "Summary_Details2=Summary_Details2[ Summary_Details2['Likes']>=Minimun_No_Likes]\n",
    "Summary_Details2=Summary_Details2[ Summary_Details2['Retweets']>=Minimun_No_Retweets]\n",
    "\n",
    "Summary_Details2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K81sHrmZtID9"
   },
   "source": [
    "For more personalized filtering you can modify the pandas dataframe directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-BDEuBH7LPTn"
   },
   "source": [
    "### Save configuration into a file\n",
    "All the IDs are read into a single set in the previous code block using the specified configuration. The ID Output file stores all the IDs in a single file so that the configuration blocks don't have to be run again. In case the program unexpectedly stops, you can just run the code for Initialization and then the code for Hydration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellView": "form",
    "id": "y4EfPTrnLO0c"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Summary_Details2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-5e3a80e517cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# The set of IDs is stored in this file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_tweet_ids_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w+\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mSummary_Details2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Tweet_ID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Summary_Details2' is not defined"
     ]
    }
   ],
   "source": [
    "#@title Enter ID output file {run: \"auto\"}\n",
    "os.chdir(working_directory)\n",
    "final_tweet_ids_filename = \"final_ids.txt\" #@param {type: \"string\"}\n",
    "# The set of IDs is stored in this file.\n",
    "with open(final_tweet_ids_filename, \"w+\") as f:\n",
    "    for id in Summary_Details2['Tweet_ID']:\n",
    "        f.write('%s\\n' % id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NxFa0jOTKbBw"
   },
   "source": [
    "# Hydrate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kBBH-a4WK1JM"
   },
   "source": [
    "### Set up output file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U74uUnemI6h9"
   },
   "source": [
    "The final_tweet_ids_filename should be exactly the same as the ID output file from the Configuration block. If this file does not exist in the working directory, you have to re-run the Configuration block.\n",
    "\n",
    "Please also keep the output_filename the same in case the code is halted. That way, tweets already hydrated aren't re-hydrated for no reason. \n",
    "\n",
    "Also, please do not remove the .txt file created after running the Hydrate block until all the data is converted to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "form",
    "id": "9ATxyEfSLBK1"
   },
   "outputs": [],
   "source": [
    "#@title Set up Directory { run: \"auto\"}\n",
    "final_tweet_ids_filename = \"final_ids.txt\" #@param {type: \"string\"}\n",
    "output_filename = \"output.csv\" #@param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QG9cS-aoW0Wy"
   },
   "source": [
    "The time for this code will depend on how many tweets you want to “hydrate”. Also, be advise of the Tweet API limit, the code will “go to sleep” once the limit is reach and automatically continue. \n",
    "You can leave this code running in Google Colab for a max of 12hrs. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "DFwYd7m58WR3",
    "outputId": "fc18b6da-9a7e-4896-e6f8-d15b97f8a669"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-10-f627ad6ad952>, line 52)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-f627ad6ad952>\"\u001b[0;36m, line \u001b[0;32m52\u001b[0m\n\u001b[0;31m    ''''\u001b[0m\n\u001b[0m                   \n^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "import jsonlines, json\n",
    "# Stores hydrated tweets here as jsonl objects\n",
    "# Contains one json object per line\n",
    "output_json_filename = output_filename[:output_filename.index(\".\")] + \".txt\"\n",
    "ids = []\n",
    "with open(final_tweet_ids_filename, \"r\") as ids_file:\n",
    "    ids = ids_file.read().split()\n",
    "hydrated_tweets = []\n",
    "ids_to_hydrate = set(ids)\n",
    "\n",
    "# Looks at the output file for already hydrated tweets\n",
    "if os.path.isfile(output_json_filename):\n",
    "    with jsonlines.open(output_json_filename, \"r\") as reader:\n",
    "        for i in reader.iter(type=dict, skip_invalid=True):\n",
    "            # These tweets have already been hydrated. So remove them from ids_to_hydrate\n",
    "            hydrated_tweets.append(i)\n",
    "            ids_to_hydrate.remove(i[\"id_str\"])\n",
    "print(\"Total IDs: \" + str(len(ids)) + \", IDs to hydrate: \" + str(len(ids_to_hydrate)))\n",
    "print(\"Hydrated: \" + str(len(hydrated_tweets)))\n",
    "\n",
    "count = len(hydrated_tweets)\n",
    "start_index = count # The index from where tweets haven't been saved to the output_json_file\n",
    "# Stores hydrated tweets to output_json_file every num_save iterations.\n",
    "num_save  = 1000\n",
    "\n",
    "# Now, use twarc and start hydrating\n",
    "for tweet in t.hydrate(ids_to_hydrate):\n",
    "    hydrated_tweets.append(tweet)\n",
    "    count += 1\n",
    "    # If num_save iterations have passed,\n",
    "    if (count % num_save) == 0:\n",
    "        # Open the output file\n",
    "        # NOTE: Even if the code stops during IO, only tweets from the current iteration are lost.\n",
    "        # Older tweets are preserved as the file is written in append mode.\n",
    "        with jsonlines.open(output_json_filename, \"a\") as writer:\n",
    "            print(\"Started IO\")\n",
    "            # Now write the tweets from start_index. The other tweets don't have to be written\n",
    "            # as they were already written in a previous iteration or run.\n",
    "            for hydrated_tweet in hydrated_tweets[start_index:]:\n",
    "                writer.write(hydrated_tweet)\n",
    "            print(\"Finished IO\")\n",
    "        print(\"Saved \" + str(count) + \" hydrated tweets.\")\n",
    "        # Now, since everything has been written. Reset start_index\n",
    "        start_index = count\n",
    "# There might be tweets unwritten in the last iteration if the count is not a multiple of num_tweets.\n",
    "# In that case, just write out the remainder of tweets.\n",
    "if count != start_index:\n",
    "    print(\"Here with start_index\", start_index)\n",
    "    with jsonlines.open(output_json_filename, \"a\") as writer:\n",
    "        for hydrated_tweet in hydrated_tweets[start_index:]:\n",
    "           writer.write(hydrated_tweet)           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X6O-KWKnbcGt"
   },
   "source": [
    "## Convert JSONL to CSV\n",
    "Data is stored in  output_json_file from the previous code block. This now converts the jsonl .txt file into a csv file. Note that the column names required is stored as a list in the code.\n",
    "\n",
    "Note that a few of the columns are actually json objects (for example, user or entities). You will have to clean these objects into 1D data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "0x68cvh5AJCD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output.txt\n"
     ]
    }
   ],
   "source": [
    "# Convert jsonl to csv\n",
    "import csv, jsonlines\n",
    "output_json_filename = output_filename[:output_filename.index(\".\")] + \".txt\"\n",
    "print(output_json_filename)\n",
    "# These are the column name that are selected to be stored in the csv\n",
    "'''''keyset = [\"created_at\", \"id\", \"id_str\", \"full_text\", \"source\", \"truncated\", \"in_reply_to_status_id\",\n",
    "          \"in_reply_to_status_id_str\", \"in_reply_to_user_id\", \"in_reply_to_user_id_str\", \n",
    "          \"in_reply_to_screen_name\", \"user\", \"coordinates\", \"place\", \"quoted_status_id\",\n",
    "          \"quoted_status_id_str\", \"is_quote_status\", \"quoted_status\", \"retweeted_status\", \n",
    "          \"quote_count\", \"reply_count\", \"retweet_count\", \"favorite_count\", \"entities\", \n",
    "          \"extended_entities\", \"favorited\", \"retweeted\", \"possibly_sensitive\", \"filter_level\", \n",
    "          \"lang\", \"matching_rules\", \"current_user_retweet\", \"scopes\", \"withheld_copyright\", \n",
    "          \"withheld_in_countries\", \"withheld_scope\", \"geo\", \"contributors\", \"display_text_range\",\n",
    "          \"quoted_status_permalink\"]\n",
    "'''''\n",
    "keyset = [\"full_text\"]\n",
    "hydrated_tweets = []\n",
    "# Reads the current tweets\n",
    "counter = 0\n",
    "with jsonlines.open(output_json_filename, \"r\") as reader:\n",
    "    for i in reader.iter(type=dict, skip_invalid=True):\n",
    "        dic_aux = {'full_text': i['full_text']}\n",
    "        hydrated_tweets.append(dic_aux)\n",
    "# Writes them out\n",
    "with  open(output_filename, \"w+\") as output_file:\n",
    "    d = csv.DictWriter(output_file, keyset)\n",
    "    d.writeheader()\n",
    "    d.writerows(hydrated_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "utvrm8g7NHNs"
   },
   "source": [
    "Your data is now stored in output_filename. If you want to re-run  this notebook, please copy output_filename file to a different directory."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "RDzd7FUKFviv"
   ],
   "name": "Cópia de Automatically_Hydrate_TweetsIDs_COVID19_v2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
